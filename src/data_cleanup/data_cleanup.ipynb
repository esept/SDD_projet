{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessed Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.read_csv(\"../data_preprocess/pp_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocessed.shape\n",
    "# df_preprocessed.dtypes\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_data(df):\n",
    "    missing_values = df_preprocessed.isna().mean() * 100\n",
    "    return missing_values.sort_values(ascending = False)\n",
    "\n",
    "missing_values = get_missing_data(df_preprocessed)\n",
    "print(\"Missing values in percentage (%)\")\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with more than 50% missing values given that they are not crucial for the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 50\n",
    "columns_to_drop = missing_values[missing_values > threshold].index\n",
    "df_preprocessed.drop(columns=columns_to_drop, inplace=True)\n",
    "df_preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename / uniformize columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.rename(columns={'Name': 'name', 'Create_Time': 'created_at', 'Downloads': 'downloads', 'Library_Name': 'library_name', 'Pipeline_tag': 'pipeline_tag'}, inplace = True)\n",
    "df_preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_name = df_preprocessed['name'].str.extract(\"([a-zA-Z0-9]+)[/](.*)\")\n",
    "extracted_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed['model_name'] = extracted_name[0].astype(str)\n",
    "df_preprocessed['model_version'] = extracted_name[1].astype(str)\n",
    "\n",
    "# print(df_preprocessed['model_name'].isna().sum(), ',', df_preprocessed['model_version'].isna().sum())\n",
    "df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_preprocessed.duplicated().sum()\n",
    "df_preprocessed['created_at'] = pd.to_datetime(df_preprocessed['created_at'])\n",
    "df_preprocessed['created_at'] = df_preprocessed['created_at'].dt.date\n",
    "df_preprocessed['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocessed['private'].unique()\n",
    "# df_preprocessed['private'].value_counts()\n",
    "# df_preprocessed.drop(columns='private', inplace=True)\n",
    "\n",
    "# df_preprocessed['downloads']\n",
    "# df_preprocessed['likes']\n",
    "# df_preprocessed[\"library_name\"]\n",
    "# df_preprocessed[[\"name\", \"library_name\"]]\n",
    "\n",
    "df_preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup Tags, tags, pipeline_tag\n",
    "\n",
    "Delete library_name, tags, pipeline_tag and newtags as all the informations can be found in the Tags column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocessed['tags'].isna().sum()\n",
    "# df_preprocessed['tags'].iloc[0].isna().sum()\n",
    "# df_preprocessed['tags'].iloc[1].isna().sum()\n",
    "\n",
    "# df_preprocessed['Tags'].iloc[0]\n",
    "# df_preprocessed.iloc[0]\n",
    "# df_preprocessed['tags'].unique()\n",
    "\n",
    "# df.shape\n",
    "# df_preprocessed.dtypes.head(45)\n",
    "df_preprocessed.drop(columns=['tags','pipeline_tag','newtags'], inplace=True)\n",
    "df_preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifing how name is printed\n",
    "- Delete df_preprocessed['name'] (given that it can be obtained by using model_name and model_version)\n",
    "- Moving model_name and model_version as the first and second columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_preprocessed.drop(columns='name', inplace=True)\n",
    "def move_columns(df, column_to_move, desired_position):\n",
    "    column_to_move_series = df.pop(column_to_move)\n",
    "    df.insert(desired_position, column_to_move, column_to_move_series)\n",
    "\n",
    "move_columns(df_preprocessed, 'model_name', 0)\n",
    "move_columns(df_preprocessed, 'model_version', 1)\n",
    "\n",
    "df_preprocessed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed['langs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isunknow(lang):\n",
    "    if lang == ',':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_preprocessed['langs'].apply(lambda x: isunknow(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of models differents languages, instead of a all-in-one string seperated by comma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocessed['langs'].str.replace(',', ' ').str.split().apply(lambda x: [i.strip() for i in x]).value_counts().index\n",
    "# df_preprocessed['langs'] =\n",
    "# df_preprocessed['langs'].str.replace(',', ' ').str.split().apply(lambda x: [i.strip() for i in x]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop not crucial columns which haven't interesting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preprocessed['private'].value_counts() # same value for all rows\n",
    "# df_preprocessed['nb_license'].value_counts() # not useful 0, 1, 2 number of licenses\n",
    "# df_preprocessed['nb_adapterhub'].value_counts() # not useful (a part 7 models, all are 0)\n",
    "# df_preprocessed['nb_template'].value_counts() # nearly 90% have 0 templates\n",
    "# df_preprocessed['nb_inference'].value_counts() # nearly 99% have 0 inferences\n",
    "# df_preprocessed['nb_region'].value_counts() # all have 1 region\n",
    "# df_preprocessed['nb_arxiv'].value_counts()?\n",
    "# df_preprocessed['nb_pipeline'].value_counts() # nearly 99% have 0 pipelines\n",
    "# df_preprocessed['nb_diffusers'].value_counts() # nearly 99% have 0 diffusers\n",
    "# df_preprocessed['nb_doi'].value_counts() # nearly 99% have 0 dois\n",
    "# df_preprocessed['nb_adapterhub'].value_counts() # nearly 99% have 0 dois\n",
    "\n",
    "df_preprocessed.drop(columns=['private', 'nb_license', 'nb_adapterhub', 'nb_template', 'nb_inference', 'nb_region', 'nb_arxiv', 'nb_pipeline', 'nb_diffusers', 'nb_doi', 'nb_adapterhub'], inplace=True)\n",
    "df_preprocessed\n",
    "\n",
    "# OK for frameworks\n",
    "# OK NB_\n",
    "# OK ONEHOT_safetensors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about how we can properly sanitize df_preprocessed['Tags'] such as all it's values aren't useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed['Tags'][0]\n",
    "# df_preprocessed['Tags'].str.replace('[','').str.replace(']','').str.replace('\\'','').str.split(',').apply(lambda x: [i.strip() for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_preprocessed.to_csv(\"../data_cleanup/clean_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
